/*
	MIT License http://www.opensource.org/licenses/mit-license.php
	Author Tobias Koppers @sokra
*/

"use strict";

const { SyncBailHook ***REMOVED*** = require("tapable");
const { RawSource, CachedSource, CompatSource ***REMOVED*** = require("webpack-sources");
const Compilation = require("../Compilation");
const WebpackError = require("../WebpackError");
const { compareSelect, compareStrings ***REMOVED*** = require("../util/comparators");
const createHash = require("../util/createHash");

/** @typedef {import("webpack-sources").Source***REMOVED*** Source */
/** @typedef {import("../Cache").Etag***REMOVED*** Etag */
/** @typedef {import("../Compilation").AssetInfo***REMOVED*** AssetInfo */
/** @typedef {import("../Compiler")***REMOVED*** Compiler */
/** @typedef {typeof import("../util/Hash")***REMOVED*** Hash */

const EMPTY_SET = new Set();

/**
 * @template T
 * @param {T | T[]***REMOVED*** itemOrItems item or items
 * @param {Set<T>***REMOVED*** list list
 */
const addToList = (itemOrItems, list) => {
	if (Array.isArray(itemOrItems)) {
		for (const item of itemOrItems) {
			list.add(item);
		***REMOVED***
	***REMOVED*** else if (itemOrItems) {
		list.add(itemOrItems);
	***REMOVED***
***REMOVED***;

/**
 * @template T
 * @param {T[]***REMOVED*** input list
 * @param {(item: T) => Buffer***REMOVED*** fn map function
 * @returns {Buffer[]***REMOVED*** buffers without duplicates
 */
const mapAndDeduplicateBuffers = (input, fn) => {
	// Buffer.equals compares size first so this should be efficient enough
	// If it becomes a performance problem we can use a map and group by size
	// instead of looping over all assets.
	const result = [];
	outer: for (const value of input) {
		const buf = fn(value);
		for (const other of result) {
			if (buf.equals(other)) continue outer;
		***REMOVED***
		result.push(buf);
	***REMOVED***
	return result;
***REMOVED***;

/**
 * Escapes regular expression metacharacters
 * @param {string***REMOVED*** str String to quote
 * @returns {string***REMOVED*** Escaped string
 */
const quoteMeta = str => str.replace(/[-[\]\\/{***REMOVED***()*+?.^$|]/g, "\\$&");

const cachedSourceMap = new WeakMap();

/**
 * @param {Source***REMOVED*** source source
 * @returns {CachedSource***REMOVED*** cached source
 */
const toCachedSource = source => {
	if (source instanceof CachedSource) {
		return source;
	***REMOVED***
	const entry = cachedSourceMap.get(source);
	if (entry !== undefined) return entry;
	const newSource = new CachedSource(CompatSource.from(source));
	cachedSourceMap.set(source, newSource);
	return newSource;
***REMOVED***;

/** @typedef {Set<string>***REMOVED*** OwnHashes */
/** @typedef {Set<string>***REMOVED*** ReferencedHashes */
/** @typedef {Set<string>***REMOVED*** Hashes */

/**
 * @typedef {object***REMOVED*** AssetInfoForRealContentHash
 * @property {string***REMOVED*** name
 * @property {AssetInfo***REMOVED*** info
 * @property {Source***REMOVED*** source
 * @property {RawSource | undefined***REMOVED*** newSource
 * @property {RawSource | undefined***REMOVED*** newSourceWithoutOwn
 * @property {string***REMOVED*** content
 * @property {OwnHashes | undefined***REMOVED*** ownHashes
 * @property {Promise<void> | undefined***REMOVED*** contentComputePromise
 * @property {Promise<void> | undefined***REMOVED*** contentComputeWithoutOwnPromise
 * @property {ReferencedHashes | undefined***REMOVED*** referencedHashes
 * @property {Hashes***REMOVED*** hashes
 */

/**
 * @typedef {object***REMOVED*** CompilationHooks
 * @property {SyncBailHook<[Buffer[], string], string | void>***REMOVED*** updateHash
 */

/** @type {WeakMap<Compilation, CompilationHooks>***REMOVED*** */
const compilationHooksMap = new WeakMap();

/**
 * @typedef {object***REMOVED*** RealContentHashPluginOptions
 * @property {string | Hash***REMOVED*** hashFunction the hash function to use
 * @property {string=***REMOVED*** hashDigest the hash digest to use
 */

const PLUGIN_NAME = "RealContentHashPlugin";

class RealContentHashPlugin {
	/**
	 * @param {Compilation***REMOVED*** compilation the compilation
	 * @returns {CompilationHooks***REMOVED*** the attached hooks
	 */
	static getCompilationHooks(compilation) {
		if (!(compilation instanceof Compilation)) {
			throw new TypeError(
				"The 'compilation' argument must be an instance of Compilation"
			);
		***REMOVED***
		let hooks = compilationHooksMap.get(compilation);
		if (hooks === undefined) {
			hooks = {
				updateHash: new SyncBailHook(["content", "oldHash"])
			***REMOVED***;
			compilationHooksMap.set(compilation, hooks);
		***REMOVED***
		return hooks;
	***REMOVED***

	/**
	 * @param {RealContentHashPluginOptions***REMOVED*** options options
	 */
	constructor({ hashFunction, hashDigest ***REMOVED***) {
		this._hashFunction = hashFunction;
		this._hashDigest = hashDigest;
	***REMOVED***

	/**
	 * Apply the plugin
	 * @param {Compiler***REMOVED*** compiler the compiler instance
	 * @returns {void***REMOVED***
	 */
	apply(compiler) {
		compiler.hooks.compilation.tap(PLUGIN_NAME, compilation => {
			const cacheAnalyse = compilation.getCache(
				"RealContentHashPlugin|analyse"
			);
			const cacheGenerate = compilation.getCache(
				"RealContentHashPlugin|generate"
			);
			const hooks = RealContentHashPlugin.getCompilationHooks(compilation);
			compilation.hooks.processAssets.tapPromise(
				{
					name: PLUGIN_NAME,
					stage: Compilation.PROCESS_ASSETS_STAGE_OPTIMIZE_HASH
				***REMOVED***,
				async () => {
					const assets = compilation.getAssets();
					/** @type {AssetInfoForRealContentHash[]***REMOVED*** */
					const assetsWithInfo = [];
					/** @type {Map<string, [AssetInfoForRealContentHash]>***REMOVED*** */
					const hashToAssets = new Map();
					for (const { source, info, name ***REMOVED*** of assets) {
						const cachedSource = toCachedSource(source);
						const content = /** @type {string***REMOVED*** */ (cachedSource.source());
						/** @type {Hashes***REMOVED*** */
						const hashes = new Set();
						addToList(info.contenthash, hashes);
						/** @type {AssetInfoForRealContentHash***REMOVED*** */
						const data = {
							name,
							info,
							source: cachedSource,
							newSource: undefined,
							newSourceWithoutOwn: undefined,
							content,
							ownHashes: undefined,
							contentComputePromise: undefined,
							contentComputeWithoutOwnPromise: undefined,
							referencedHashes: undefined,
							hashes
						***REMOVED***;
						assetsWithInfo.push(data);
						for (const hash of hashes) {
							const list = hashToAssets.get(hash);
							if (list === undefined) {
								hashToAssets.set(hash, [data]);
							***REMOVED*** else {
								list.push(data);
							***REMOVED***
						***REMOVED***
					***REMOVED***
					if (hashToAssets.size === 0) return;
					const hashRegExp = new RegExp(
						Array.from(hashToAssets.keys(), quoteMeta).join("|"),
						"g"
					);
					await Promise.all(
						assetsWithInfo.map(async asset => {
							const { name, source, content, hashes ***REMOVED*** = asset;
							if (Buffer.isBuffer(content)) {
								asset.referencedHashes = EMPTY_SET;
								asset.ownHashes = EMPTY_SET;
								return;
							***REMOVED***
							const etag = cacheAnalyse.mergeEtags(
								cacheAnalyse.getLazyHashedEtag(source),
								Array.from(hashes).join("|")
							);
							[asset.referencedHashes, asset.ownHashes] =
								await cacheAnalyse.providePromise(name, etag, () => {
									const referencedHashes = new Set();
									const ownHashes = new Set();
									const inContent = content.match(hashRegExp);
									if (inContent) {
										for (const hash of inContent) {
											if (hashes.has(hash)) {
												ownHashes.add(hash);
												continue;
											***REMOVED***
											referencedHashes.add(hash);
										***REMOVED***
									***REMOVED***
									return [referencedHashes, ownHashes];
								***REMOVED***);
						***REMOVED***)
					);
					/**
					 * @param {string***REMOVED*** hash the hash
					 * @returns {undefined | ReferencedHashes***REMOVED*** the referenced hashes
					 */
					const getDependencies = hash => {
						const assets = hashToAssets.get(hash);
						if (!assets) {
							const referencingAssets = assetsWithInfo.filter(asset =>
								/** @type {ReferencedHashes***REMOVED*** */ (asset.referencedHashes).has(
									hash
								)
							);
							const err = new WebpackError(`RealContentHashPlugin
Some kind of unexpected caching problem occurred.
An asset was cached with a reference to another asset (${hash***REMOVED***) that's not in the compilation anymore.
Either the asset was incorrectly cached, or the referenced asset should also be restored from cache.
Referenced by:
${referencingAssets
	.map(a => {
		const match = new RegExp(`.{0,20***REMOVED***${quoteMeta(hash)***REMOVED***.{0,20***REMOVED***`).exec(
			a.content
		);
		return ` - ${a.name***REMOVED***: ...${match ? match[0] : "???"***REMOVED***...`;
	***REMOVED***)
	.join("\n")***REMOVED***`);
							compilation.errors.push(err);
							return;
						***REMOVED***
						const hashes = new Set();
						for (const { referencedHashes, ownHashes ***REMOVED*** of assets) {
							if (!(/** @type {OwnHashes***REMOVED*** */ (ownHashes).has(hash))) {
								for (const hash of /** @type {OwnHashes***REMOVED*** */ (ownHashes)) {
									hashes.add(hash);
								***REMOVED***
							***REMOVED***
							for (const hash of /** @type {ReferencedHashes***REMOVED*** */ (
								referencedHashes
							)) {
								hashes.add(hash);
							***REMOVED***
						***REMOVED***
						return hashes;
					***REMOVED***;
					/**
					 * @param {string***REMOVED*** hash the hash
					 * @returns {string***REMOVED*** the hash info
					 */
					const hashInfo = hash => {
						const assets = hashToAssets.get(hash);
						return `${hash***REMOVED*** (${Array.from(
							/** @type {AssetInfoForRealContentHash[]***REMOVED*** */ (assets),
							a => a.name
						)***REMOVED***)`;
					***REMOVED***;
					const hashesInOrder = new Set();
					for (const hash of hashToAssets.keys()) {
						/**
						 * @param {string***REMOVED*** hash the hash
						 * @param {Set<string>***REMOVED*** stack stack of hashes
						 */
						const add = (hash, stack) => {
							const deps = getDependencies(hash);
							if (!deps) return;
							stack.add(hash);
							for (const dep of deps) {
								if (hashesInOrder.has(dep)) continue;
								if (stack.has(dep)) {
									throw new Error(
										`Circular hash dependency ${Array.from(
											stack,
											hashInfo
										).join(" -> ")***REMOVED*** -> ${hashInfo(dep)***REMOVED***`
									);
								***REMOVED***
								add(dep, stack);
							***REMOVED***
							hashesInOrder.add(hash);
							stack.delete(hash);
						***REMOVED***;
						if (hashesInOrder.has(hash)) continue;
						add(hash, new Set());
					***REMOVED***
					const hashToNewHash = new Map();
					/**
					 * @param {AssetInfoForRealContentHash***REMOVED*** asset asset info
					 * @returns {Etag***REMOVED*** etag
					 */
					const getEtag = asset =>
						cacheGenerate.mergeEtags(
							cacheGenerate.getLazyHashedEtag(asset.source),
							Array.from(
								/** @type {ReferencedHashes***REMOVED*** */ (asset.referencedHashes),
								hash => hashToNewHash.get(hash)
							).join("|")
						);
					/**
					 * @param {AssetInfoForRealContentHash***REMOVED*** asset asset info
					 * @returns {Promise<void>***REMOVED***
					 */
					const computeNewContent = asset => {
						if (asset.contentComputePromise) return asset.contentComputePromise;
						return (asset.contentComputePromise = (async () => {
							if (
								/** @type {OwnHashes***REMOVED*** */ (asset.ownHashes).size > 0 ||
								Array.from(
									/** @type {ReferencedHashes***REMOVED*** */
									(asset.referencedHashes)
								).some(hash => hashToNewHash.get(hash) !== hash)
							) {
								const identifier = asset.name;
								const etag = getEtag(asset);
								asset.newSource = await cacheGenerate.providePromise(
									identifier,
									etag,
									() => {
										const newContent = asset.content.replace(hashRegExp, hash =>
											hashToNewHash.get(hash)
										);
										return new RawSource(newContent);
									***REMOVED***
								);
							***REMOVED***
						***REMOVED***)());
					***REMOVED***;
					/**
					 * @param {AssetInfoForRealContentHash***REMOVED*** asset asset info
					 * @returns {Promise<void>***REMOVED***
					 */
					const computeNewContentWithoutOwn = asset => {
						if (asset.contentComputeWithoutOwnPromise)
							return asset.contentComputeWithoutOwnPromise;
						return (asset.contentComputeWithoutOwnPromise = (async () => {
							if (
								/** @type {OwnHashes***REMOVED*** */ (asset.ownHashes).size > 0 ||
								Array.from(
									/** @type {ReferencedHashes***REMOVED*** */
									(asset.referencedHashes)
								).some(hash => hashToNewHash.get(hash) !== hash)
							) {
								const identifier = `${asset.name***REMOVED***|without-own`;
								const etag = getEtag(asset);
								asset.newSourceWithoutOwn = await cacheGenerate.providePromise(
									identifier,
									etag,
									() => {
										const newContent = asset.content.replace(
											hashRegExp,
											hash => {
												if (
													/** @type {OwnHashes***REMOVED*** */ (asset.ownHashes).has(hash)
												) {
													return "";
												***REMOVED***
												return hashToNewHash.get(hash);
											***REMOVED***
										);
										return new RawSource(newContent);
									***REMOVED***
								);
							***REMOVED***
						***REMOVED***)());
					***REMOVED***;
					const comparator = compareSelect(a => a.name, compareStrings);
					for (const oldHash of hashesInOrder) {
						const assets =
							/** @type {AssetInfoForRealContentHash[]***REMOVED*** */
							(hashToAssets.get(oldHash));
						assets.sort(comparator);
						await Promise.all(
							assets.map(asset =>
								/** @type {OwnHashes***REMOVED*** */ (asset.ownHashes).has(oldHash)
									? computeNewContentWithoutOwn(asset)
									: computeNewContent(asset)
							)
						);
						const assetsContent = mapAndDeduplicateBuffers(assets, asset => {
							if (/** @type {OwnHashes***REMOVED*** */ (asset.ownHashes).has(oldHash)) {
								return asset.newSourceWithoutOwn
									? asset.newSourceWithoutOwn.buffer()
									: asset.source.buffer();
							***REMOVED***
							return asset.newSource
								? asset.newSource.buffer()
								: asset.source.buffer();
						***REMOVED***);
						let newHash = hooks.updateHash.call(assetsContent, oldHash);
						if (!newHash) {
							const hash = createHash(this._hashFunction);
							if (compilation.outputOptions.hashSalt) {
								hash.update(compilation.outputOptions.hashSalt);
							***REMOVED***
							for (const content of assetsContent) {
								hash.update(content);
							***REMOVED***
							const digest = hash.digest(this._hashDigest);
							newHash = /** @type {string***REMOVED*** */ (digest.slice(0, oldHash.length));
						***REMOVED***
						hashToNewHash.set(oldHash, newHash);
					***REMOVED***
					await Promise.all(
						assetsWithInfo.map(async asset => {
							await computeNewContent(asset);
							const newName = asset.name.replace(hashRegExp, hash =>
								hashToNewHash.get(hash)
							);

							const infoUpdate = {***REMOVED***;
							const hash = asset.info.contenthash;
							infoUpdate.contenthash = Array.isArray(hash)
								? hash.map(hash => hashToNewHash.get(hash))
								: hashToNewHash.get(hash);

							if (asset.newSource !== undefined) {
								compilation.updateAsset(
									asset.name,
									asset.newSource,
									infoUpdate
								);
							***REMOVED*** else {
								compilation.updateAsset(asset.name, asset.source, infoUpdate);
							***REMOVED***

							if (asset.name !== newName) {
								compilation.renameAsset(asset.name, newName);
							***REMOVED***
						***REMOVED***)
					);
				***REMOVED***
			);
		***REMOVED***);
	***REMOVED***
***REMOVED***

module.exports = RealContentHashPlugin;
