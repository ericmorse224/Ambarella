/*
	MIT License http://www.opensource.org/licenses/mit-license.php
*/

"use strict";

const { constants ***REMOVED*** = require("buffer");
const { pipeline ***REMOVED*** = require("stream");
const {
	createBrotliCompress,
	createBrotliDecompress,
	createGzip,
	createGunzip,
	constants: zConstants
***REMOVED*** = require("zlib");
const { DEFAULTS ***REMOVED*** = require("../config/defaults");
const createHash = require("../util/createHash");
const { dirname, join, mkdirp ***REMOVED*** = require("../util/fs");
const memoize = require("../util/memoize");
const SerializerMiddleware = require("./SerializerMiddleware");

/** @typedef {typeof import("../util/Hash")***REMOVED*** Hash */
/** @typedef {import("../util/fs").IStats***REMOVED*** IStats */
/** @typedef {import("../util/fs").IntermediateFileSystem***REMOVED*** IntermediateFileSystem */
/** @typedef {import("./types").BufferSerializableType***REMOVED*** BufferSerializableType */

/*
Format:

File -> Header Section*

Version -> u32
AmountOfSections -> u32
SectionSize -> i32 (if less than zero represents lazy value)

Header -> Version AmountOfSections SectionSize*

Buffer -> n bytes
Section -> Buffer

*/

// "wpc" + 1 in little-endian
const VERSION = 0x01637077;
const WRITE_LIMIT_TOTAL = 0x7fff0000;
const WRITE_LIMIT_CHUNK = 511 * 1024 * 1024;

/**
 * @param {Buffer[]***REMOVED*** buffers buffers
 * @param {string | Hash***REMOVED*** hashFunction hash function to use
 * @returns {string***REMOVED*** hash
 */
const hashForName = (buffers, hashFunction) => {
	const hash = createHash(hashFunction);
	for (const buf of buffers) hash.update(buf);
	return /** @type {string***REMOVED*** */ (hash.digest("hex"));
***REMOVED***;

const COMPRESSION_CHUNK_SIZE = 100 * 1024 * 1024;
const DECOMPRESSION_CHUNK_SIZE = 100 * 1024 * 1024;

/** @type {(buffer: Buffer, value: number, offset: number) => void***REMOVED*** */
const writeUInt64LE = Buffer.prototype.writeBigUInt64LE
	? (buf, value, offset) => {
			buf.writeBigUInt64LE(BigInt(value), offset);
		***REMOVED***
	: (buf, value, offset) => {
			const low = value % 0x100000000;
			const high = (value - low) / 0x100000000;
			buf.writeUInt32LE(low, offset);
			buf.writeUInt32LE(high, offset + 4);
		***REMOVED***;

/** @type {(buffer: Buffer, offset: number) => void***REMOVED*** */
const readUInt64LE = Buffer.prototype.readBigUInt64LE
	? (buf, offset) => Number(buf.readBigUInt64LE(offset))
	: (buf, offset) => {
			const low = buf.readUInt32LE(offset);
			const high = buf.readUInt32LE(offset + 4);
			return high * 0x100000000 + low;
		***REMOVED***;

/** @typedef {Promise<void | void[]>***REMOVED*** BackgroundJob */

/**
 * @typedef {object***REMOVED*** SerializeResult
 * @property {string | false***REMOVED*** name
 * @property {number***REMOVED*** size
 * @property {BackgroundJob=***REMOVED*** backgroundJob
 */

/** @typedef {{ name: string, size: number ***REMOVED******REMOVED*** LazyOptions */
/**
 * @typedef {import("./SerializerMiddleware").LazyFunction<BufferSerializableType[], Buffer, FileMiddleware, LazyOptions>***REMOVED*** LazyFunction
 */

/**
 * @param {FileMiddleware***REMOVED*** middleware this
 * @param {(BufferSerializableType | LazyFunction)[]***REMOVED*** data data to be serialized
 * @param {string | boolean***REMOVED*** name file base name
 * @param {(name: string | false, buffers: Buffer[], size: number) => Promise<void>***REMOVED*** writeFile writes a file
 * @param {string | Hash***REMOVED*** hashFunction hash function to use
 * @returns {Promise<SerializeResult>***REMOVED*** resulting file pointer and promise
 */
const serialize = async (
	middleware,
	data,
	name,
	writeFile,
	hashFunction = DEFAULTS.HASH_FUNCTION
) => {
	/** @type {(Buffer[] | Buffer | Promise<SerializeResult>)[]***REMOVED*** */
	const processedData = [];
	/** @type {WeakMap<SerializeResult, LazyFunction>***REMOVED*** */
	const resultToLazy = new WeakMap();
	/** @type {Buffer[] | undefined***REMOVED*** */
	let lastBuffers;
	for (const item of await data) {
		if (typeof item === "function") {
			if (!SerializerMiddleware.isLazy(item))
				throw new Error("Unexpected function");
			if (!SerializerMiddleware.isLazy(item, middleware)) {
				throw new Error(
					"Unexpected lazy value with non-this target (can't pass through lazy values)"
				);
			***REMOVED***
			lastBuffers = undefined;
			const serializedInfo = SerializerMiddleware.getLazySerializedValue(item);
			if (serializedInfo) {
				if (typeof serializedInfo === "function") {
					throw new Error(
						"Unexpected lazy value with non-this target (can't pass through lazy values)"
					);
				***REMOVED*** else {
					processedData.push(serializedInfo);
				***REMOVED***
			***REMOVED*** else {
				const content = item();
				if (content) {
					const options = SerializerMiddleware.getLazyOptions(item);
					processedData.push(
						serialize(
							middleware,
							/** @type {BufferSerializableType[]***REMOVED*** */
							(content),
							(options && options.name) || true,
							writeFile,
							hashFunction
						).then(result => {
							/** @type {LazyOptions***REMOVED*** */
							(item.options).size = result.size;
							resultToLazy.set(result, item);
							return result;
						***REMOVED***)
					);
				***REMOVED*** else {
					throw new Error(
						"Unexpected falsy value returned by lazy value function"
					);
				***REMOVED***
			***REMOVED***
		***REMOVED*** else if (item) {
			if (lastBuffers) {
				lastBuffers.push(item);
			***REMOVED*** else {
				lastBuffers = [item];
				processedData.push(lastBuffers);
			***REMOVED***
		***REMOVED*** else {
			throw new Error("Unexpected falsy value in items array");
		***REMOVED***
	***REMOVED***
	/** @type {BackgroundJob[]***REMOVED*** */
	const backgroundJobs = [];
	const resolvedData = (await Promise.all(processedData)).map(item => {
		if (Array.isArray(item) || Buffer.isBuffer(item)) return item;

		backgroundJobs.push(
			/** @type {BackgroundJob***REMOVED*** */
			(item.backgroundJob)
		);
		// create pointer buffer from size and name
		const name = /** @type {string***REMOVED*** */ (item.name);
		const nameBuffer = Buffer.from(name);
		const buf = Buffer.allocUnsafe(8 + nameBuffer.length);
		writeUInt64LE(buf, item.size, 0);
		nameBuffer.copy(buf, 8, 0);
		const lazy =
			/** @type {LazyFunction***REMOVED*** */
			(resultToLazy.get(item));
		SerializerMiddleware.setLazySerializedValue(lazy, buf);
		return buf;
	***REMOVED***);
	/** @type {number[]***REMOVED*** */
	const lengths = [];
	for (const item of resolvedData) {
		if (Array.isArray(item)) {
			let l = 0;
			for (const b of item) l += b.length;
			while (l > 0x7fffffff) {
				lengths.push(0x7fffffff);
				l -= 0x7fffffff;
			***REMOVED***
			lengths.push(l);
		***REMOVED*** else if (item) {
			lengths.push(-item.length);
		***REMOVED*** else {
			throw new Error(`Unexpected falsy value in resolved data ${item***REMOVED***`);
		***REMOVED***
	***REMOVED***
	const header = Buffer.allocUnsafe(8 + lengths.length * 4);
	header.writeUInt32LE(VERSION, 0);
	header.writeUInt32LE(lengths.length, 4);
	for (let i = 0; i < lengths.length; i++) {
		header.writeInt32LE(lengths[i], 8 + i * 4);
	***REMOVED***
	/** @type {Buffer[]***REMOVED*** */
	const buf = [header];
	for (const item of resolvedData) {
		if (Array.isArray(item)) {
			for (const b of item) buf.push(b);
		***REMOVED*** else if (item) {
			buf.push(item);
		***REMOVED***
	***REMOVED***
	if (name === true) {
		name = hashForName(buf, hashFunction);
	***REMOVED***
	let size = 0;
	for (const b of buf) size += b.length;
	backgroundJobs.push(writeFile(name, buf, size));
	return {
		size,
		name,
		backgroundJob:
			backgroundJobs.length === 1
				? backgroundJobs[0]
				: /** @type {BackgroundJob***REMOVED*** */ (Promise.all(backgroundJobs))
	***REMOVED***;
***REMOVED***;

/**
 * @param {FileMiddleware***REMOVED*** middleware this
 * @param {string | false***REMOVED*** name filename
 * @param {(name: string | false) => Promise<Buffer[]>***REMOVED*** readFile read content of a file
 * @returns {Promise<BufferSerializableType[]>***REMOVED*** deserialized data
 */
const deserialize = async (middleware, name, readFile) => {
	const contents = await readFile(name);
	if (contents.length === 0) throw new Error(`Empty file ${name***REMOVED***`);
	let contentsIndex = 0;
	let contentItem = contents[0];
	let contentItemLength = contentItem.length;
	let contentPosition = 0;
	if (contentItemLength === 0) throw new Error(`Empty file ${name***REMOVED***`);
	const nextContent = () => {
		contentsIndex++;
		contentItem = contents[contentsIndex];
		contentItemLength = contentItem.length;
		contentPosition = 0;
	***REMOVED***;
	/**
	 * @param {number***REMOVED*** n number of bytes to ensure
	 */
	const ensureData = n => {
		if (contentPosition === contentItemLength) {
			nextContent();
		***REMOVED***
		while (contentItemLength - contentPosition < n) {
			const remaining = contentItem.slice(contentPosition);
			let lengthFromNext = n - remaining.length;
			const buffers = [remaining];
			for (let i = contentsIndex + 1; i < contents.length; i++) {
				const l = contents[i].length;
				if (l > lengthFromNext) {
					buffers.push(contents[i].slice(0, lengthFromNext));
					contents[i] = contents[i].slice(lengthFromNext);
					lengthFromNext = 0;
					break;
				***REMOVED*** else {
					buffers.push(contents[i]);
					contentsIndex = i;
					lengthFromNext -= l;
				***REMOVED***
			***REMOVED***
			if (lengthFromNext > 0) throw new Error("Unexpected end of data");
			contentItem = Buffer.concat(buffers, n);
			contentItemLength = n;
			contentPosition = 0;
		***REMOVED***
	***REMOVED***;
	/**
	 * @returns {number***REMOVED*** value value
	 */
	const readUInt32LE = () => {
		ensureData(4);
		const value = contentItem.readUInt32LE(contentPosition);
		contentPosition += 4;
		return value;
	***REMOVED***;
	/**
	 * @returns {number***REMOVED*** value value
	 */
	const readInt32LE = () => {
		ensureData(4);
		const value = contentItem.readInt32LE(contentPosition);
		contentPosition += 4;
		return value;
	***REMOVED***;
	/**
	 * @param {number***REMOVED*** l length
	 * @returns {Buffer***REMOVED*** buffer
	 */
	const readSlice = l => {
		ensureData(l);
		if (contentPosition === 0 && contentItemLength === l) {
			const result = contentItem;
			if (contentsIndex + 1 < contents.length) {
				nextContent();
			***REMOVED*** else {
				contentPosition = l;
			***REMOVED***
			return result;
		***REMOVED***
		const result = contentItem.slice(contentPosition, contentPosition + l);
		contentPosition += l;
		// we clone the buffer here to allow the original content to be garbage collected
		return l * 2 < contentItem.buffer.byteLength ? Buffer.from(result) : result;
	***REMOVED***;
	const version = readUInt32LE();
	if (version !== VERSION) {
		throw new Error("Invalid file version");
	***REMOVED***
	const sectionCount = readUInt32LE();
	const lengths = [];
	let lastLengthPositive = false;
	for (let i = 0; i < sectionCount; i++) {
		const value = readInt32LE();
		const valuePositive = value >= 0;
		if (lastLengthPositive && valuePositive) {
			lengths[lengths.length - 1] += value;
		***REMOVED*** else {
			lengths.push(value);
			lastLengthPositive = valuePositive;
		***REMOVED***
	***REMOVED***
	/** @type {BufferSerializableType[]***REMOVED*** */
	const result = [];
	for (let length of lengths) {
		if (length < 0) {
			const slice = readSlice(-length);
			const size = Number(readUInt64LE(slice, 0));
			const nameBuffer = slice.slice(8);
			const name = nameBuffer.toString();
			const lazy =
				/** @type {LazyFunction***REMOVED*** */
				(
					SerializerMiddleware.createLazy(
						memoize(() => deserialize(middleware, name, readFile)),
						middleware,
						{ name, size ***REMOVED***,
						slice
					)
				);
			result.push(lazy);
		***REMOVED*** else {
			if (contentPosition === contentItemLength) {
				nextContent();
			***REMOVED*** else if (contentPosition !== 0) {
				if (length <= contentItemLength - contentPosition) {
					result.push(
						Buffer.from(
							contentItem.buffer,
							contentItem.byteOffset + contentPosition,
							length
						)
					);
					contentPosition += length;
					length = 0;
				***REMOVED*** else {
					const l = contentItemLength - contentPosition;
					result.push(
						Buffer.from(
							contentItem.buffer,
							contentItem.byteOffset + contentPosition,
							l
						)
					);
					length -= l;
					contentPosition = contentItemLength;
				***REMOVED***
			***REMOVED*** else if (length >= contentItemLength) {
				result.push(contentItem);
				length -= contentItemLength;
				contentPosition = contentItemLength;
			***REMOVED*** else {
				result.push(
					Buffer.from(contentItem.buffer, contentItem.byteOffset, length)
				);
				contentPosition += length;
				length = 0;
			***REMOVED***
			while (length > 0) {
				nextContent();
				if (length >= contentItemLength) {
					result.push(contentItem);
					length -= contentItemLength;
					contentPosition = contentItemLength;
				***REMOVED*** else {
					result.push(
						Buffer.from(contentItem.buffer, contentItem.byteOffset, length)
					);
					contentPosition += length;
					length = 0;
				***REMOVED***
			***REMOVED***
		***REMOVED***
	***REMOVED***
	return result;
***REMOVED***;

/** @typedef {BufferSerializableType[]***REMOVED*** DeserializedType */
/** @typedef {true***REMOVED*** SerializedType */
/** @typedef {{ filename: string, extension?: string ***REMOVED******REMOVED*** Context */

/**
 * @extends {SerializerMiddleware<DeserializedType, SerializedType, Context>***REMOVED***
 */
class FileMiddleware extends SerializerMiddleware {
	/**
	 * @param {IntermediateFileSystem***REMOVED*** fs filesystem
	 * @param {string | Hash***REMOVED*** hashFunction hash function to use
	 */
	constructor(fs, hashFunction = DEFAULTS.HASH_FUNCTION) {
		super();
		this.fs = fs;
		this._hashFunction = hashFunction;
	***REMOVED***

	/**
	 * @param {DeserializedType***REMOVED*** data data
	 * @param {Context***REMOVED*** context context object
	 * @returns {SerializedType | Promise<SerializedType> | null***REMOVED*** serialized data
	 */
	serialize(data, context) {
		const { filename, extension = "" ***REMOVED*** = context;
		return new Promise((resolve, reject) => {
			mkdirp(this.fs, dirname(this.fs, filename), err => {
				if (err) return reject(err);

				// It's important that we don't touch existing files during serialization
				// because serialize may read existing files (when deserializing)
				const allWrittenFiles = new Set();
				/**
				 * @param {string | false***REMOVED*** name name
				 * @param {Buffer[]***REMOVED*** content content
				 * @param {number***REMOVED*** size size
				 * @returns {Promise<void>***REMOVED***
				 */
				const writeFile = async (name, content, size) => {
					const file = name
						? join(this.fs, filename, `../${name***REMOVED***${extension***REMOVED***`)
						: filename;
					await new Promise(
						/**
						 * @param {(value?: undefined) => void***REMOVED*** resolve resolve
						 * @param {(reason?: Error | null) => void***REMOVED*** reject reject
						 */
						(resolve, reject) => {
							let stream = this.fs.createWriteStream(`${file***REMOVED***_`);
							let compression;
							if (file.endsWith(".gz")) {
								compression = createGzip({
									chunkSize: COMPRESSION_CHUNK_SIZE,
									level: zConstants.Z_BEST_SPEED
								***REMOVED***);
							***REMOVED*** else if (file.endsWith(".br")) {
								compression = createBrotliCompress({
									chunkSize: COMPRESSION_CHUNK_SIZE,
									params: {
										[zConstants.BROTLI_PARAM_MODE]: zConstants.BROTLI_MODE_TEXT,
										[zConstants.BROTLI_PARAM_QUALITY]: 2,
										[zConstants.BROTLI_PARAM_DISABLE_LITERAL_CONTEXT_MODELING]: true,
										[zConstants.BROTLI_PARAM_SIZE_HINT]: size
									***REMOVED***
								***REMOVED***);
							***REMOVED***
							if (compression) {
								pipeline(compression, stream, reject);
								stream = compression;
								stream.on("finish", () => resolve());
							***REMOVED*** else {
								stream.on("error", err => reject(err));
								stream.on("finish", () => resolve());
							***REMOVED***
							// split into chunks for WRITE_LIMIT_CHUNK size
							/** @type {Buffer[]***REMOVED*** */
							const chunks = [];
							for (const b of content) {
								if (b.length < WRITE_LIMIT_CHUNK) {
									chunks.push(b);
								***REMOVED*** else {
									for (let i = 0; i < b.length; i += WRITE_LIMIT_CHUNK) {
										chunks.push(b.slice(i, i + WRITE_LIMIT_CHUNK));
									***REMOVED***
								***REMOVED***
							***REMOVED***

							const len = chunks.length;
							let i = 0;
							/**
							 * @param {(Error | null)=***REMOVED*** err err
							 */
							const batchWrite = err => {
								// will be handled in "on" error handler
								if (err) return;

								if (i === len) {
									stream.end();
									return;
								***REMOVED***

								// queue up a batch of chunks up to the write limit
								// end is exclusive
								let end = i;
								let sum = chunks[end++].length;
								while (end < len) {
									sum += chunks[end].length;
									if (sum > WRITE_LIMIT_TOTAL) break;
									end++;
								***REMOVED***
								while (i < end - 1) {
									stream.write(chunks[i++]);
								***REMOVED***
								stream.write(chunks[i++], batchWrite);
							***REMOVED***;
							batchWrite();
						***REMOVED***
					);
					if (name) allWrittenFiles.add(file);
				***REMOVED***;

				resolve(
					serialize(this, data, false, writeFile, this._hashFunction).then(
						async ({ backgroundJob ***REMOVED***) => {
							await backgroundJob;

							// Rename the index file to disallow access during inconsistent file state
							await new Promise(
								/**
								 * @param {(value?: undefined) => void***REMOVED*** resolve resolve
								 */
								resolve => {
									this.fs.rename(filename, `${filename***REMOVED***.old`, err => {
										resolve();
									***REMOVED***);
								***REMOVED***
							);

							// update all written files
							await Promise.all(
								Array.from(
									allWrittenFiles,
									file =>
										new Promise(
											/**
											 * @param {(value?: undefined) => void***REMOVED*** resolve resolve
											 * @param {(reason?: Error | null) => void***REMOVED*** reject reject
											 * @returns {void***REMOVED***
											 */
											(resolve, reject) => {
												this.fs.rename(`${file***REMOVED***_`, file, err => {
													if (err) return reject(err);
													resolve();
												***REMOVED***);
											***REMOVED***
										)
								)
							);

							// As final step automatically update the index file to have a consistent pack again
							await new Promise(
								/**
								 * @param {(value?: undefined) => void***REMOVED*** resolve resolve
								 * @returns {void***REMOVED***
								 */
								resolve => {
									this.fs.rename(`${filename***REMOVED***_`, filename, err => {
										if (err) return reject(err);
										resolve();
									***REMOVED***);
								***REMOVED***
							);
							return /** @type {true***REMOVED*** */ (true);
						***REMOVED***
					)
				);
			***REMOVED***);
		***REMOVED***);
	***REMOVED***

	/**
	 * @param {SerializedType***REMOVED*** data data
	 * @param {Context***REMOVED*** context context object
	 * @returns {DeserializedType | Promise<DeserializedType>***REMOVED*** deserialized data
	 */
	deserialize(data, context) {
		const { filename, extension = "" ***REMOVED*** = context;
		/**
		 * @param {string | boolean***REMOVED*** name name
		 * @returns {Promise<Buffer[]>***REMOVED*** result
		 */
		const readFile = name =>
			new Promise((resolve, reject) => {
				const file = name
					? join(this.fs, filename, `../${name***REMOVED***${extension***REMOVED***`)
					: filename;
				this.fs.stat(file, (err, stats) => {
					if (err) {
						reject(err);
						return;
					***REMOVED***
					let remaining = /** @type {IStats***REMOVED*** */ (stats).size;
					/** @type {Buffer | undefined***REMOVED*** */
					let currentBuffer;
					/** @type {number | undefined***REMOVED*** */
					let currentBufferUsed;
					/** @type {Buffer[]***REMOVED*** */
					const buf = [];
					/** @type {import("zlib").Zlib & import("stream").Transform | undefined***REMOVED*** */
					let decompression;
					if (file.endsWith(".gz")) {
						decompression = createGunzip({
							chunkSize: DECOMPRESSION_CHUNK_SIZE
						***REMOVED***);
					***REMOVED*** else if (file.endsWith(".br")) {
						decompression = createBrotliDecompress({
							chunkSize: DECOMPRESSION_CHUNK_SIZE
						***REMOVED***);
					***REMOVED***
					if (decompression) {
						/** @typedef {(value: Buffer[] | PromiseLike<Buffer[]>) => void***REMOVED*** NewResolve */
						/** @typedef {(reason?: Error) => void***REMOVED*** NewReject */

						/** @type {NewResolve | undefined***REMOVED*** */
						let newResolve;
						/** @type {NewReject | undefined***REMOVED*** */
						let newReject;
						resolve(
							Promise.all([
								new Promise((rs, rj) => {
									newResolve = rs;
									newReject = rj;
								***REMOVED***),
								new Promise(
									/**
									 * @param {(value?: undefined) => void***REMOVED*** resolve resolve
									 * @param {(reason?: Error) => void***REMOVED*** reject reject
									 */
									(resolve, reject) => {
										decompression.on("data", chunk => buf.push(chunk));
										decompression.on("end", () => resolve());
										decompression.on("error", err => reject(err));
									***REMOVED***
								)
							]).then(() => buf)
						);
						resolve = /** @type {NewResolve***REMOVED*** */ (newResolve);
						reject = /** @type {NewReject***REMOVED*** */ (newReject);
					***REMOVED***
					this.fs.open(file, "r", (err, _fd) => {
						if (err) {
							reject(err);
							return;
						***REMOVED***
						const fd = /** @type {number***REMOVED*** */ (_fd);
						const read = () => {
							if (currentBuffer === undefined) {
								currentBuffer = Buffer.allocUnsafeSlow(
									Math.min(
										constants.MAX_LENGTH,
										remaining,
										decompression ? DECOMPRESSION_CHUNK_SIZE : Infinity
									)
								);
								currentBufferUsed = 0;
							***REMOVED***
							let readBuffer = currentBuffer;
							let readOffset = /** @type {number***REMOVED*** */ (currentBufferUsed);
							let readLength =
								currentBuffer.length -
								/** @type {number***REMOVED*** */ (currentBufferUsed);
							// values passed to fs.read must be valid int32 values
							if (readOffset > 0x7fffffff) {
								readBuffer = currentBuffer.slice(readOffset);
								readOffset = 0;
							***REMOVED***
							if (readLength > 0x7fffffff) {
								readLength = 0x7fffffff;
							***REMOVED***
							this.fs.read(
								fd,
								readBuffer,
								readOffset,
								readLength,
								null,
								(err, bytesRead) => {
									if (err) {
										this.fs.close(fd, () => {
											reject(err);
										***REMOVED***);
										return;
									***REMOVED***
									/** @type {number***REMOVED*** */
									(currentBufferUsed) += bytesRead;
									remaining -= bytesRead;
									if (
										currentBufferUsed ===
										/** @type {Buffer***REMOVED*** */
										(currentBuffer).length
									) {
										if (decompression) {
											decompression.write(currentBuffer);
										***REMOVED*** else {
											buf.push(
												/** @type {Buffer***REMOVED*** */
												(currentBuffer)
											);
										***REMOVED***
										currentBuffer = undefined;
										if (remaining === 0) {
											if (decompression) {
												decompression.end();
											***REMOVED***
											this.fs.close(fd, err => {
												if (err) {
													reject(err);
													return;
												***REMOVED***
												resolve(buf);
											***REMOVED***);
											return;
										***REMOVED***
									***REMOVED***
									read();
								***REMOVED***
							);
						***REMOVED***;
						read();
					***REMOVED***);
				***REMOVED***);
			***REMOVED***);
		return deserialize(this, false, readFile);
	***REMOVED***
***REMOVED***

module.exports = FileMiddleware;
